{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import featurevis\n",
    "from featurevis import models\n",
    "from featurevis import ops\n",
    "from featurevis import utils\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cpu': \n",
    "    print('Running models on CPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get a pre-trained model for neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from staticnet_analyses import multi_mei\n",
    "from staticnet_experiments import models as static_models\n",
    "\n",
    "key = {'data_hash': '7572eed73113c993e7d1b92f83e270b4', 'group_id': 29, \n",
    "       'net_hash': '80d0d4bc112470b2ba04cd5eba048e39', 'neuron_id': 119, \n",
    "       'readout_key': 'group029-21067-9-17-0'}\n",
    "\n",
    "# Get our f (average of four models)\n",
    "train_stats = multi_mei.prepare_data(key, key['readout_key'])\n",
    "_, (_, _, height, width), _, mean_behavior, mean_eyepos, _ = train_stats\n",
    "model_key = {'group_id': key['group_id'], 'net_hash': key['net_hash']}\n",
    "my_models = [(static_models.Model & mk).load_network() for mk in (static_models.Model & model_key).proj()]\n",
    "model = models.Ensemble(my_models, key['readout_key'], eye_pos=mean_eyepos, neuron_idx=key['neuron_id'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = train_stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.images[dset.tiers == 'train'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image = torch.randn(1, 1, 36, 64, dtype=torch.float32, device=device)  # grayscale random image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD (no bells and whistles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, step_size=5, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM (no bells and whistles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, optim_name='Adam', step_size=0.1, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiCarlo (Bashivan et al., 2018)\n",
    "See Sec. *Synthesized \"controller\" images* in p.9.\n",
    "* Optimizer: SGD\n",
    "* Transform: Jittering\n",
    "* Regularization: Total variation\n",
    "* Gradient function: Normalize the gradient (grad / norm(grad)) and clip between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_transform = ops.Jitter(max_jitter=(2, 4))\n",
    "dc_regularization = ops.TotalVariation(weight=0.001)\n",
    "dc_gradient = utils.Compose([ops.ChangeNorm(1), ops.ClipRange(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, step_size=1, num_iterations=700, \n",
    "                                                       transform=dc_transform, regularization=dc_regularization, \n",
    "                                                       gradient_f=dc_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(7*3, 4))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].plot(reg_values)\n",
    "axes[2].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepTune (Abbasi-Asl, 2018)\n",
    "See Equation in p.8\n",
    "* Optimizer: SGD\n",
    "* Regularization: total variation and l6 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_regularization = utils.Combine([ops.TotalVariation(weight=0.001), ops.LpNorm(weight=1, p=6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, step_size=1, num_iterations=500, \n",
    "                                                       regularization=dt_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(7*3, 4))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].plot(reg_values)\n",
    "axes[2].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walker et al., 2019\n",
    "* Optimizer: SGD\n",
    "* Gradient: Fourier smoothing, divide by mean of absolute gradient and multiply by a decaying learning rate\n",
    "* Post update: Clip range and blur image with a decaying sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walker_gradient = utils.Compose([ops.FourierSmoothing(0.04), # not exactly the same as fft_smooth(precond=0.1) but close\n",
    "                                 ops.DivideByMeanOfAbsolute(),\n",
    "                                 ops.MultiplyBy(1/850, decay_factor=(1/850 - 1/20400) /(1-1000))])  # decays from 1/850 to 1/20400 in 1000 iterations\n",
    "bias, scale = 111.28329467773438, 60.922306060791016\n",
    "walker_postup = utils.Compose([ops.ClipRange(-bias / scale, (255 - bias) / scale), \n",
    "                               ops.GaussianBlur(1.5, decay_factor=(1.5 - 0.01) /(1-1000))]) # decays from 1.5 to 0.01 in 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, step_size=1, num_iterations=1000, \n",
    "                                                       post_update=walker_postup, gradient_f=walker_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mei = (multi_mei.MEI & key).fetch1('mei')\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(mei)\n",
    "plt.title('MEI from deepdraw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative MEI generation\n",
    "TODO: Finding the simplest way to generate robust MEIs\n",
    "* Optimizer: SGD\n",
    "* Post update: Keep std to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_postup = ops.ChangeStd(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, step_size=1, num_iterations=1000, \n",
    "                                                       post_update=alt_postup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEI (least exciting image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lei_model = utils.Compose([model, ops.MultiplyBy(-1)]) # negative model\n",
    "lei_postup = ops.ChangeStd(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(lei_model, initial_image, step_size=1, num_iterations=1000, \n",
    "                                                        post_update=lei_postup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking evolution of MEI\n",
    "Just to show `save_iters`. Something similar could be used to run early stopping (by testing intermediate MEIs in a validation model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, step_size=10, num_iterations=100, save_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 6, figsize=(20, 3))\n",
    "axes[0].set_title('Iter {} f(x) = {:.2f}'.format(0, fevals[0]))\n",
    "axes[0].imshow(initial_image.squeeze().detach().cpu().numpy())\n",
    "for ax, i, one_x in zip(axes[1:], range(20, 101, 20), opt_x):\n",
    "    ax.imshow(one_x.squeeze().detach().cpu().numpy())\n",
    "    ax.set_title('Iter {}: f(x) = {:.2f}'.format(i, fevals[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diverse MEIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_batch = torch.randn(5, 1, 36, 64, dtype=torch.float32, device=device) # 5 grayscale random images\n",
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask that contains the relevant part of the image (if available)\n",
    "from staticnet_analyses import largefov\n",
    "mask = torch.as_tensor((largefov.MEIMask & key).fetch1('mask'), dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity in pixel space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_regularization = ops.Similarity(10, mask=mask, metric='correlation')\n",
    "div_postup = ops.ChangeStd(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_batch, step_size=10, num_iterations=500, \n",
    "                                                       regularization=div_regularization, post_update=div_postup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].plot(reg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(opt_x), figsize=(20, 3))\n",
    "for ax, one_x in zip(axes, opt_x):\n",
    "    ax.imshow(one_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity in (VGG-19) feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "mini_mask = F.avg_pool2d(mask.unsqueeze(0).unsqueeze(0), kernel_size=4).squeeze() # the VGG features get downsampled twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_regularization = utils.Compose([ops.GrayscaleToRGB(), models.VGG19Core(layer=15), \n",
    "                                    ops.Similarity(0.02, mask=mini_mask, metric='euclidean')])\n",
    "div_postup = ops.ChangeStd(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_batch, step_size=10, num_iterations=500, \n",
    "                                                       regularization=div_regularization, post_update=div_postup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].plot(reg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(opt_x), figsize=(20, 3))\n",
    "for ax, one_x in zip(axes, opt_x):\n",
    "    ax.imshow(one_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texture\n",
    "### Random crops\n",
    "* Optimizer: SGD\n",
    "* Transform: Take a random crop from the big image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image2 = torch.randn(1, 1, 36*2, 64*2, dtype=torch.float32, device=device)\n",
    "text_transform = ops.RandomCrop(36, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image2, step_size=5, num_iterations=5000, transform=text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched random crops (a la Santiago)\n",
    "* Optimizer: SGD\n",
    "* Transform: Create a batch with overlapping tiles of the big image, optimize the mean activity overall\n",
    "\n",
    "Doesn't look as nice. Also, I ran out of memory for bigger FOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image2 = torch.randn(1, 1, 36 + 18, 64 + 31, dtype=torch.float32, device=device)\n",
    "text_transform = ops.BatchedCrops(36, 64, step_size=5, sigma=(8, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image2, step_size=10, num_iterations=400, transform=text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "axes[0].plot(fevals)\n",
    "axes[1].imshow(opt_x.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  download a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.VGG19(layer=14, channel=13, device=device) # conv 3_1, feature map 13\n",
    "model = models.VGG19(layer=40, channel=150, device=device) # conv 5_1, feature map 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image = torch.randn(1, 3, 128, 128, dtype=torch.float32, device=device) # 128 x 128 RGB image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(model, initial_image, optim_name='Adam', step_size=0.1, num_iterations=1000, \n",
    "                                                       transform=ops.Jitter(3), # jitter to avoid adversarial effects\n",
    "                                                       gradient_f=ops.GaussianBlur(1), # bit of blurring on gradient to avoid high freq effects\n",
    "                                                       post_update=ops.ChangeStd(1)) # keep the image in a reasonable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "axes[0].plot(fevals)\n",
    "im = opt_x.squeeze().transpose(0, -1).detach().cpu().numpy()\n",
    "im = (im - im.min(axis=(0, 1))) / (im.max(axis=(0, 1)) - im.min(axis=(0, 1)))\n",
    "axes[1].imshow(im)\n",
    "axes[2].imshow(opt_x.squeeze().transpose(0, -1).detach().cpu().numpy() / 2 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lei_model = utils.Compose([model, ops.MultiplyBy(-1)]) # find least activating features \n",
    "opt_x, fevals, reg_values = featurevis.gradient_ascent(lei_model, initial_image, optim_name='Adam', step_size=0.1, num_iterations=1000, \n",
    "                                                       transform=ops.Jitter(3), # jitter to avoid adversarial effects\n",
    "                                                       gradient_f=ops.GaussianBlur(1), # bit of blurring on gradient to avoid high freq effects\n",
    "                                                       post_update=ops.ChangeStd(1)) # keep the image in a reasonable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7))\n",
    "axes[0].plot(fevals)\n",
    "im = opt_x.squeeze().transpose(0, -1).detach().cpu().numpy()\n",
    "im = (im - im.min(axis=(0, 1))) / (im.max(axis=(0, 1)) - im.min(axis=(0, 1)))\n",
    "axes[1].imshow(im)\n",
    "axes[2].imshow(opt_x.squeeze().transpose(0, -1).detach().cpu().numpy() / 3 + 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
